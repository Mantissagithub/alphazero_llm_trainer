# training hyperparameters for AlphaZero LLM Trainer (10-HOUR OPTIMIZED)

# MCTS Parameters
mcts:
  num_iterations: 60               # REDUCED from 100 (saves 40% time)
  exploration_constant: 1.414      # Kept at sqrt(2) - optimal
  max_tree_depth: 12               # REDUCED from 15 (saves 20% time)
  tokens_per_expansion: 20         # Kept same
  right_tokens: 10                 # Kept same
  wrong_tokens: 10                 # Kept same

# reward system config
rewards:
  hre_weight: 0.4
  pre_weight: 0.6
  pre_accumulation: true
  pre_discount_factor: 0.95

  # hard reward estimator settings
  hre:
    terminal_reward: 1.0
    incorrect_penalty: 0.0
    use_fuzzy_matching: false

  # perplexity reward estimator settings
  pre:
    similarity_threshold: 0.8
    num_teachers_for_eval: 3       # REDUCED from 5 (saves 40% eval time)
    perplexity_scale: "log"

# training loop settings
training:
  batch_size: 8
  gradient_accumulation_steps: 4
  num_epochs: 1                    # REDUCED from 3 (saves 66% training time)
  learning_rate: 2.0e-4
  warmup_steps: 50                 # REDUCED from 100 (faster warmup)
  max_grad_norm: 1.0
  weight_decay: 0.01

  # optimizer settings
  optimizer:
    type: "adamw_8bit"
    betas: [0.9, 0.999]
    eps: 1e-8

  # learning rate scheduler settings
  scheduler:
    type: "cosine"
    num_cycles: 0.5

# student model training settings
student_training:
  enabled: true
  epsilon_start: 0.5
  epsilon_end: 0.1
  epsilon_decay: 0.995
  min_reward_threshold: 0.5
  learning_rate: 2e-5

  # GRPO settings
  grpo:
    enabled: true
    beta: 0.1
    normalize_advantages: true
    clip_advantages: 3.0
    min_group_size: 1
    use_legacy_loss: false
    micro_batch_size: 2  # Process trajectories in smaller batches to save GPU memory (reduced to prevent OOM)

  # kl penalty settings
  kl_penalty:
    enabled: true
    kl_weight: 0.01
    update_ref_every: 50

# dataset settings
dataset:
  name: "openai/gsm8k"
  split: "main"
  train_size: 50
  eval_size: 20
  max_length: 512

  # preprocessing settings
  preprocessing:
    shuffle: true
    remove_duplicates: true
    seed: 42

# logging and checkpointing
logging:
  log_interval: 10
  eval_interval: 50                # MORE FREQUENT from 100
  save_interval: 300               # CHECKPOINTS at 300 and 600

  wandb:
    enabled: false
    project_name: "alphazero-llm-trainer"
    entity: "vidhyaprakash2510"

checkpointing:
  save_dir: "./checkpoints"
  keep_best: 3
  metric: "reward"

# hardware settings
hardware:
  use_fp16: false
  use_bf16: true                   # H100 optimized
  gradient_checkpointing: true

# api settings for teacher models
api:
  openrouter_base_url: "https://openrouter.ai/api/v1"
  max_retries: 3
  timeout: 30
  rate_limit_delay: 0.1

  # Cost management
  max_cost_per_sample: 0.01
  use_free_models_only: true
